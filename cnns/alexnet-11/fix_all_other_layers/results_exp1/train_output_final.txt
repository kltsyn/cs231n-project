I0313 19:17:02.463855 12045 caffe.cpp:99] Use GPU with device ID 0
I0313 19:17:02.821959 12045 caffe.cpp:107] Starting Optimization
I0313 19:17:02.822149 12045 solver.cpp:32] Initializing solver from parameters: 
test_iter: 60
test_interval: 200
base_lr: 0.01
display: 100
max_iter: 2000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 200
snapshot: 200
snapshot_prefix: "/root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers"
solver_mode: GPU
net: "/root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/train_val.prototxt"
I0313 19:17:02.822199 12045 solver.cpp:70] Creating training net from net file: /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/train_val.prototxt
I0313 19:17:02.822899 12045 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0313 19:17:02.822938 12045 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0313 19:17:02.823127 12045 net.cpp:42] Initializing net from parameters: 
name: "alexnet11"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/root/cs231n-project/data/images/train/no_augmentations/imagenet/256/train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-alexnet11"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-alexnet11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-alexnet11"
  bottom: "label"
  top: "loss"
}
I0313 19:17:02.823318 12045 layer_factory.hpp:74] Creating layer data
I0313 19:17:02.823351 12045 net.cpp:76] Creating Layer data
I0313 19:17:02.823374 12045 net.cpp:334] data -> data
I0313 19:17:02.823429 12045 net.cpp:334] data -> label
I0313 19:17:02.823451 12045 net.cpp:105] Setting up data
I0313 19:17:02.823568 12045 db.cpp:34] Opened lmdb /root/cs231n-project/data/images/train/no_augmentations/imagenet/256/train_lmdb
I0313 19:17:02.823943 12045 data_layer.cpp:67] output data size: 100,3,227,227
I0313 19:17:02.823978 12045 data_transformer.cpp:22] Loading mean file from: /root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto
I0313 19:17:02.854473 12045 net.cpp:112] Top shape: 100 3 227 227 (15458700)
I0313 19:17:02.854548 12045 net.cpp:112] Top shape: 100 1 1 1 (100)
I0313 19:17:02.854573 12045 layer_factory.hpp:74] Creating layer conv1
I0313 19:17:02.854605 12045 net.cpp:76] Creating Layer conv1
I0313 19:17:02.854624 12045 net.cpp:372] conv1 <- data
I0313 19:17:02.854660 12045 net.cpp:334] conv1 -> conv1
I0313 19:17:02.854691 12045 net.cpp:105] Setting up conv1
I0313 19:17:02.856130 12045 net.cpp:112] Top shape: 100 96 55 55 (29040000)
I0313 19:17:02.856178 12045 layer_factory.hpp:74] Creating layer relu1
I0313 19:17:02.856196 12045 net.cpp:76] Creating Layer relu1
I0313 19:17:02.856211 12045 net.cpp:372] relu1 <- conv1
I0313 19:17:02.856230 12045 net.cpp:323] relu1 -> conv1 (in-place)
I0313 19:17:02.856245 12045 net.cpp:105] Setting up relu1
I0313 19:17:02.856261 12045 net.cpp:112] Top shape: 100 96 55 55 (29040000)
I0313 19:17:02.856271 12045 layer_factory.hpp:74] Creating layer pool1
I0313 19:17:02.856287 12045 net.cpp:76] Creating Layer pool1
I0313 19:17:02.856294 12045 net.cpp:372] pool1 <- conv1
I0313 19:17:02.856304 12045 net.cpp:334] pool1 -> pool1
I0313 19:17:02.856317 12045 net.cpp:105] Setting up pool1
I0313 19:17:02.856379 12045 net.cpp:112] Top shape: 100 96 27 27 (6998400)
I0313 19:17:02.856400 12045 layer_factory.hpp:74] Creating layer norm1
I0313 19:17:02.856425 12045 net.cpp:76] Creating Layer norm1
I0313 19:17:02.856446 12045 net.cpp:372] norm1 <- pool1
I0313 19:17:02.856457 12045 net.cpp:334] norm1 -> norm1
I0313 19:17:02.856489 12045 net.cpp:105] Setting up norm1
I0313 19:17:02.856559 12045 net.cpp:112] Top shape: 100 96 27 27 (6998400)
I0313 19:17:02.856580 12045 layer_factory.hpp:74] Creating layer conv2
I0313 19:17:02.856593 12045 net.cpp:76] Creating Layer conv2
I0313 19:17:02.856601 12045 net.cpp:372] conv2 <- norm1
I0313 19:17:02.856612 12045 net.cpp:334] conv2 -> conv2
I0313 19:17:02.856623 12045 net.cpp:105] Setting up conv2
I0313 19:17:02.869279 12045 net.cpp:112] Top shape: 100 256 27 27 (18662400)
I0313 19:17:02.869313 12045 layer_factory.hpp:74] Creating layer relu2
I0313 19:17:02.869326 12045 net.cpp:76] Creating Layer relu2
I0313 19:17:02.869334 12045 net.cpp:372] relu2 <- conv2
I0313 19:17:02.869346 12045 net.cpp:323] relu2 -> conv2 (in-place)
I0313 19:17:02.869364 12045 net.cpp:105] Setting up relu2
I0313 19:17:02.869408 12045 net.cpp:112] Top shape: 100 256 27 27 (18662400)
I0313 19:17:02.869426 12045 layer_factory.hpp:74] Creating layer pool2
I0313 19:17:02.869449 12045 net.cpp:76] Creating Layer pool2
I0313 19:17:02.869462 12045 net.cpp:372] pool2 <- conv2
I0313 19:17:02.869472 12045 net.cpp:334] pool2 -> pool2
I0313 19:17:02.869482 12045 net.cpp:105] Setting up pool2
I0313 19:17:02.869494 12045 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:17:02.869503 12045 layer_factory.hpp:74] Creating layer norm2
I0313 19:17:02.869518 12045 net.cpp:76] Creating Layer norm2
I0313 19:17:02.869529 12045 net.cpp:372] norm2 <- pool2
I0313 19:17:02.869537 12045 net.cpp:334] norm2 -> norm2
I0313 19:17:02.869592 12045 net.cpp:105] Setting up norm2
I0313 19:17:02.869608 12045 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:17:02.869618 12045 layer_factory.hpp:74] Creating layer conv3
I0313 19:17:02.869633 12045 net.cpp:76] Creating Layer conv3
I0313 19:17:02.869642 12045 net.cpp:372] conv3 <- norm2
I0313 19:17:02.869652 12045 net.cpp:334] conv3 -> conv3
I0313 19:17:02.869664 12045 net.cpp:105] Setting up conv3
I0313 19:17:02.899554 12045 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:17:02.899642 12045 layer_factory.hpp:74] Creating layer relu3
I0313 19:17:02.899662 12045 net.cpp:76] Creating Layer relu3
I0313 19:17:02.899672 12045 net.cpp:372] relu3 <- conv3
I0313 19:17:02.899682 12045 net.cpp:323] relu3 -> conv3 (in-place)
I0313 19:17:02.899698 12045 net.cpp:105] Setting up relu3
I0313 19:17:02.899711 12045 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:17:02.899725 12045 layer_factory.hpp:74] Creating layer conv4
I0313 19:17:02.899750 12045 net.cpp:76] Creating Layer conv4
I0313 19:17:02.899775 12045 net.cpp:372] conv4 <- conv3
I0313 19:17:02.899791 12045 net.cpp:334] conv4 -> conv4
I0313 19:17:02.899802 12045 net.cpp:105] Setting up conv4
I0313 19:17:02.922291 12045 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:17:02.922359 12045 layer_factory.hpp:74] Creating layer relu4
I0313 19:17:02.922376 12045 net.cpp:76] Creating Layer relu4
I0313 19:17:02.922385 12045 net.cpp:372] relu4 <- conv4
I0313 19:17:02.922416 12045 net.cpp:323] relu4 -> conv4 (in-place)
I0313 19:17:02.922433 12045 net.cpp:105] Setting up relu4
I0313 19:17:02.922448 12045 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:17:02.922463 12045 layer_factory.hpp:74] Creating layer conv5
I0313 19:17:02.922484 12045 net.cpp:76] Creating Layer conv5
I0313 19:17:02.922500 12045 net.cpp:372] conv5 <- conv4
I0313 19:17:02.922520 12045 net.cpp:334] conv5 -> conv5
I0313 19:17:02.922534 12045 net.cpp:105] Setting up conv5
I0313 19:17:02.937712 12045 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:17:02.937788 12045 layer_factory.hpp:74] Creating layer relu5
I0313 19:17:02.937811 12045 net.cpp:76] Creating Layer relu5
I0313 19:17:02.937829 12045 net.cpp:372] relu5 <- conv5
I0313 19:17:02.937850 12045 net.cpp:323] relu5 -> conv5 (in-place)
I0313 19:17:02.937876 12045 net.cpp:105] Setting up relu5
I0313 19:17:02.937886 12045 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:17:02.937894 12045 layer_factory.hpp:74] Creating layer pool5
I0313 19:17:02.937921 12045 net.cpp:76] Creating Layer pool5
I0313 19:17:02.937945 12045 net.cpp:372] pool5 <- conv5
I0313 19:17:02.937958 12045 net.cpp:334] pool5 -> pool5
I0313 19:17:02.937968 12045 net.cpp:105] Setting up pool5
I0313 19:17:02.937979 12045 net.cpp:112] Top shape: 100 256 6 6 (921600)
I0313 19:17:02.937986 12045 layer_factory.hpp:74] Creating layer fc6
I0313 19:17:02.938011 12045 net.cpp:76] Creating Layer fc6
I0313 19:17:02.938019 12045 net.cpp:372] fc6 <- pool5
I0313 19:17:02.938029 12045 net.cpp:334] fc6 -> fc6
I0313 19:17:02.938045 12045 net.cpp:105] Setting up fc6
I0313 19:17:04.210664 12045 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:17:04.210754 12045 layer_factory.hpp:74] Creating layer relu6
I0313 19:17:04.210774 12045 net.cpp:76] Creating Layer relu6
I0313 19:17:04.210784 12045 net.cpp:372] relu6 <- fc6
I0313 19:17:04.210813 12045 net.cpp:323] relu6 -> fc6 (in-place)
I0313 19:17:04.210836 12045 net.cpp:105] Setting up relu6
I0313 19:17:04.210850 12045 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:17:04.210857 12045 layer_factory.hpp:74] Creating layer drop6
I0313 19:17:04.210875 12045 net.cpp:76] Creating Layer drop6
I0313 19:17:04.210883 12045 net.cpp:372] drop6 <- fc6
I0313 19:17:04.210892 12045 net.cpp:323] drop6 -> fc6 (in-place)
I0313 19:17:04.210907 12045 net.cpp:105] Setting up drop6
I0313 19:17:04.210927 12045 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:17:04.210943 12045 layer_factory.hpp:74] Creating layer fc7
I0313 19:17:04.210963 12045 net.cpp:76] Creating Layer fc7
I0313 19:17:04.210976 12045 net.cpp:372] fc7 <- fc6
I0313 19:17:04.210999 12045 net.cpp:334] fc7 -> fc7
I0313 19:17:04.211020 12045 net.cpp:105] Setting up fc7
I0313 19:17:04.786247 12045 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:17:04.786334 12045 layer_factory.hpp:74] Creating layer relu7
I0313 19:17:04.786360 12045 net.cpp:76] Creating Layer relu7
I0313 19:17:04.786381 12045 net.cpp:372] relu7 <- fc7
I0313 19:17:04.786412 12045 net.cpp:323] relu7 -> fc7 (in-place)
I0313 19:17:04.786429 12045 net.cpp:105] Setting up relu7
I0313 19:17:04.786437 12045 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:17:04.786445 12045 layer_factory.hpp:74] Creating layer drop7
I0313 19:17:04.786455 12045 net.cpp:76] Creating Layer drop7
I0313 19:17:04.786463 12045 net.cpp:372] drop7 <- fc7
I0313 19:17:04.786475 12045 net.cpp:323] drop7 -> fc7 (in-place)
I0313 19:17:04.786485 12045 net.cpp:105] Setting up drop7
I0313 19:17:04.786494 12045 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:17:04.786501 12045 layer_factory.hpp:74] Creating layer fc8-alexnet11
I0313 19:17:04.786515 12045 net.cpp:76] Creating Layer fc8-alexnet11
I0313 19:17:04.786526 12045 net.cpp:372] fc8-alexnet11 <- fc7
I0313 19:17:04.786540 12045 net.cpp:334] fc8-alexnet11 -> fc8-alexnet11
I0313 19:17:04.786553 12045 net.cpp:105] Setting up fc8-alexnet11
I0313 19:17:04.788168 12045 net.cpp:112] Top shape: 100 11 1 1 (1100)
I0313 19:17:04.788200 12045 layer_factory.hpp:74] Creating layer loss
I0313 19:17:04.788221 12045 net.cpp:76] Creating Layer loss
I0313 19:17:04.788235 12045 net.cpp:372] loss <- fc8-alexnet11
I0313 19:17:04.788245 12045 net.cpp:372] loss <- label
I0313 19:17:04.788264 12045 net.cpp:334] loss -> loss
I0313 19:17:04.788285 12045 net.cpp:105] Setting up loss
I0313 19:17:04.788302 12045 layer_factory.hpp:74] Creating layer loss
I0313 19:17:04.788336 12045 net.cpp:112] Top shape: 1 1 1 1 (1)
I0313 19:17:04.788357 12045 net.cpp:118]     with loss weight 1
I0313 19:17:04.788419 12045 net.cpp:163] loss needs backward computation.
I0313 19:17:04.788445 12045 net.cpp:163] fc8-alexnet11 needs backward computation.
I0313 19:17:04.788471 12045 net.cpp:165] drop7 does not need backward computation.
I0313 19:17:04.788483 12045 net.cpp:165] relu7 does not need backward computation.
I0313 19:17:04.788491 12045 net.cpp:165] fc7 does not need backward computation.
I0313 19:17:04.788504 12045 net.cpp:165] drop6 does not need backward computation.
I0313 19:17:04.788522 12045 net.cpp:165] relu6 does not need backward computation.
I0313 19:17:04.788544 12045 net.cpp:165] fc6 does not need backward computation.
I0313 19:17:04.788564 12045 net.cpp:165] pool5 does not need backward computation.
I0313 19:17:04.788573 12045 net.cpp:165] relu5 does not need backward computation.
I0313 19:17:04.788580 12045 net.cpp:165] conv5 does not need backward computation.
I0313 19:17:04.788588 12045 net.cpp:165] relu4 does not need backward computation.
I0313 19:17:04.788594 12045 net.cpp:165] conv4 does not need backward computation.
I0313 19:17:04.788610 12045 net.cpp:165] relu3 does not need backward computation.
I0313 19:17:04.788619 12045 net.cpp:165] conv3 does not need backward computation.
I0313 19:17:04.788626 12045 net.cpp:165] norm2 does not need backward computation.
I0313 19:17:04.788640 12045 net.cpp:165] pool2 does not need backward computation.
I0313 19:17:04.788648 12045 net.cpp:165] relu2 does not need backward computation.
I0313 19:17:04.788655 12045 net.cpp:165] conv2 does not need backward computation.
I0313 19:17:04.788663 12045 net.cpp:165] norm1 does not need backward computation.
I0313 19:17:04.788671 12045 net.cpp:165] pool1 does not need backward computation.
I0313 19:17:04.788677 12045 net.cpp:165] relu1 does not need backward computation.
I0313 19:17:04.788686 12045 net.cpp:165] conv1 does not need backward computation.
I0313 19:17:04.788693 12045 net.cpp:165] data does not need backward computation.
I0313 19:17:04.788700 12045 net.cpp:201] This network produces output loss
I0313 19:17:04.788722 12045 net.cpp:446] Collecting Learning Rate and Weight Decay.
I0313 19:17:04.788738 12045 net.cpp:213] Network initialization done.
I0313 19:17:04.788746 12045 net.cpp:214] Memory required for data: 686018804
I0313 19:17:04.789386 12045 solver.cpp:154] Creating test net (#0) specified by net file: /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/train_val.prototxt
I0313 19:17:04.789458 12045 net.cpp:253] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0313 19:17:04.789686 12045 net.cpp:42] Initializing net from parameters: 
name: "alexnet11"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/root/cs231n-project/data/images/val/instagram/227/val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-alexnet11"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-alexnet11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8-alexnet11"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-alexnet11"
  bottom: "label"
  top: "loss"
}
I0313 19:17:04.789836 12045 layer_factory.hpp:74] Creating layer data
I0313 19:17:04.789862 12045 net.cpp:76] Creating Layer data
I0313 19:17:04.789878 12045 net.cpp:334] data -> data
I0313 19:17:04.789892 12045 net.cpp:334] data -> label
I0313 19:17:04.789911 12045 net.cpp:105] Setting up data
I0313 19:17:04.789984 12045 db.cpp:34] Opened lmdb /root/cs231n-project/data/images/val/instagram/227/val_lmdb
I0313 19:17:04.790280 12045 data_layer.cpp:67] output data size: 50,3,227,227
I0313 19:17:04.790305 12045 data_transformer.cpp:22] Loading mean file from: /root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto
I0313 19:17:04.805650 12045 net.cpp:112] Top shape: 50 3 227 227 (7729350)
I0313 19:17:04.805727 12045 net.cpp:112] Top shape: 50 1 1 1 (50)
I0313 19:17:04.805742 12045 layer_factory.hpp:74] Creating layer label_data_1_split
I0313 19:17:04.805764 12045 net.cpp:76] Creating Layer label_data_1_split
I0313 19:17:04.805784 12045 net.cpp:372] label_data_1_split <- label
I0313 19:17:04.805824 12045 net.cpp:334] label_data_1_split -> label_data_1_split_0
I0313 19:17:04.805851 12045 net.cpp:334] label_data_1_split -> label_data_1_split_1
I0313 19:17:04.805866 12045 net.cpp:105] Setting up label_data_1_split
I0313 19:17:04.805883 12045 net.cpp:112] Top shape: 50 1 1 1 (50)
I0313 19:17:04.805892 12045 net.cpp:112] Top shape: 50 1 1 1 (50)
I0313 19:17:04.805901 12045 layer_factory.hpp:74] Creating layer conv1
I0313 19:17:04.805917 12045 net.cpp:76] Creating Layer conv1
I0313 19:17:04.805925 12045 net.cpp:372] conv1 <- data
I0313 19:17:04.805938 12045 net.cpp:334] conv1 -> conv1
I0313 19:17:04.805950 12045 net.cpp:105] Setting up conv1
I0313 19:17:04.807129 12045 net.cpp:112] Top shape: 50 96 55 55 (14520000)
I0313 19:17:04.807165 12045 layer_factory.hpp:74] Creating layer relu1
I0313 19:17:04.807179 12045 net.cpp:76] Creating Layer relu1
I0313 19:17:04.807191 12045 net.cpp:372] relu1 <- conv1
I0313 19:17:04.807207 12045 net.cpp:323] relu1 -> conv1 (in-place)
I0313 19:17:04.807240 12045 net.cpp:105] Setting up relu1
I0313 19:17:04.807251 12045 net.cpp:112] Top shape: 50 96 55 55 (14520000)
I0313 19:17:04.807260 12045 layer_factory.hpp:74] Creating layer pool1
I0313 19:17:04.807272 12045 net.cpp:76] Creating Layer pool1
I0313 19:17:04.807279 12045 net.cpp:372] pool1 <- conv1
I0313 19:17:04.807291 12045 net.cpp:334] pool1 -> pool1
I0313 19:17:04.807309 12045 net.cpp:105] Setting up pool1
I0313 19:17:04.807340 12045 net.cpp:112] Top shape: 50 96 27 27 (3499200)
I0313 19:17:04.807353 12045 layer_factory.hpp:74] Creating layer norm1
I0313 19:17:04.807366 12045 net.cpp:76] Creating Layer norm1
I0313 19:17:04.807409 12045 net.cpp:372] norm1 <- pool1
I0313 19:17:04.807438 12045 net.cpp:334] norm1 -> norm1
I0313 19:17:04.807451 12045 net.cpp:105] Setting up norm1
I0313 19:17:04.807459 12045 net.cpp:112] Top shape: 50 96 27 27 (3499200)
I0313 19:17:04.807467 12045 layer_factory.hpp:74] Creating layer conv2
I0313 19:17:04.807479 12045 net.cpp:76] Creating Layer conv2
I0313 19:17:04.807487 12045 net.cpp:372] conv2 <- norm1
I0313 19:17:04.807497 12045 net.cpp:334] conv2 -> conv2
I0313 19:17:04.807517 12045 net.cpp:105] Setting up conv2
I0313 19:17:04.817775 12045 net.cpp:112] Top shape: 50 256 27 27 (9331200)
I0313 19:17:04.817826 12045 layer_factory.hpp:74] Creating layer relu2
I0313 19:17:04.817841 12045 net.cpp:76] Creating Layer relu2
I0313 19:17:04.817848 12045 net.cpp:372] relu2 <- conv2
I0313 19:17:04.817862 12045 net.cpp:323] relu2 -> conv2 (in-place)
I0313 19:17:04.817880 12045 net.cpp:105] Setting up relu2
I0313 19:17:04.817911 12045 net.cpp:112] Top shape: 50 256 27 27 (9331200)
I0313 19:17:04.817924 12045 layer_factory.hpp:74] Creating layer pool2
I0313 19:17:04.817942 12045 net.cpp:76] Creating Layer pool2
I0313 19:17:04.817962 12045 net.cpp:372] pool2 <- conv2
I0313 19:17:04.817973 12045 net.cpp:334] pool2 -> pool2
I0313 19:17:04.817986 12045 net.cpp:105] Setting up pool2
I0313 19:17:04.817996 12045 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:17:04.818003 12045 layer_factory.hpp:74] Creating layer norm2
I0313 19:17:04.818018 12045 net.cpp:76] Creating Layer norm2
I0313 19:17:04.818028 12045 net.cpp:372] norm2 <- pool2
I0313 19:17:04.818037 12045 net.cpp:334] norm2 -> norm2
I0313 19:17:04.818053 12045 net.cpp:105] Setting up norm2
I0313 19:17:04.818063 12045 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:17:04.818070 12045 layer_factory.hpp:74] Creating layer conv3
I0313 19:17:04.818084 12045 net.cpp:76] Creating Layer conv3
I0313 19:17:04.818107 12045 net.cpp:372] conv3 <- norm2
I0313 19:17:04.818121 12045 net.cpp:334] conv3 -> conv3
I0313 19:17:04.818132 12045 net.cpp:105] Setting up conv3
I0313 19:17:04.847995 12045 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:17:04.848075 12045 layer_factory.hpp:74] Creating layer relu3
I0313 19:17:04.848093 12045 net.cpp:76] Creating Layer relu3
I0313 19:17:04.848104 12045 net.cpp:372] relu3 <- conv3
I0313 19:17:04.848124 12045 net.cpp:323] relu3 -> conv3 (in-place)
I0313 19:17:04.848160 12045 net.cpp:105] Setting up relu3
I0313 19:17:04.848211 12045 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:17:04.848223 12045 layer_factory.hpp:74] Creating layer conv4
I0313 19:17:04.848238 12045 net.cpp:76] Creating Layer conv4
I0313 19:17:04.848255 12045 net.cpp:372] conv4 <- conv3
I0313 19:17:04.848280 12045 net.cpp:334] conv4 -> conv4
I0313 19:17:04.848312 12045 net.cpp:105] Setting up conv4
I0313 19:17:04.870620 12045 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:17:04.870654 12045 layer_factory.hpp:74] Creating layer relu4
I0313 19:17:04.870668 12045 net.cpp:76] Creating Layer relu4
I0313 19:17:04.870682 12045 net.cpp:372] relu4 <- conv4
I0313 19:17:04.870717 12045 net.cpp:323] relu4 -> conv4 (in-place)
I0313 19:17:04.870734 12045 net.cpp:105] Setting up relu4
I0313 19:17:04.870761 12045 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:17:04.870774 12045 layer_factory.hpp:74] Creating layer conv5
I0313 19:17:04.870791 12045 net.cpp:76] Creating Layer conv5
I0313 19:17:04.870808 12045 net.cpp:372] conv5 <- conv4
I0313 19:17:04.870829 12045 net.cpp:334] conv5 -> conv5
I0313 19:17:04.870846 12045 net.cpp:105] Setting up conv5
I0313 19:17:04.885726 12045 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:17:04.885761 12045 layer_factory.hpp:74] Creating layer relu5
I0313 19:17:04.885774 12045 net.cpp:76] Creating Layer relu5
I0313 19:17:04.885788 12045 net.cpp:372] relu5 <- conv5
I0313 19:17:04.885808 12045 net.cpp:323] relu5 -> conv5 (in-place)
I0313 19:17:04.885825 12045 net.cpp:105] Setting up relu5
I0313 19:17:04.885834 12045 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:17:04.885843 12045 layer_factory.hpp:74] Creating layer pool5
I0313 19:17:04.885856 12045 net.cpp:76] Creating Layer pool5
I0313 19:17:04.885871 12045 net.cpp:372] pool5 <- conv5
I0313 19:17:04.885882 12045 net.cpp:334] pool5 -> pool5
I0313 19:17:04.885893 12045 net.cpp:105] Setting up pool5
I0313 19:17:04.885910 12045 net.cpp:112] Top shape: 50 256 6 6 (460800)
I0313 19:17:04.885920 12045 layer_factory.hpp:74] Creating layer fc6
I0313 19:17:04.885937 12045 net.cpp:76] Creating Layer fc6
I0313 19:17:04.885952 12045 net.cpp:372] fc6 <- pool5
I0313 19:17:04.885965 12045 net.cpp:334] fc6 -> fc6
I0313 19:17:04.885979 12045 net.cpp:105] Setting up fc6
I0313 19:17:06.158078 12045 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:17:06.158184 12045 layer_factory.hpp:74] Creating layer relu6
I0313 19:17:06.164479 12045 net.cpp:76] Creating Layer relu6
I0313 19:17:06.164523 12045 net.cpp:372] relu6 <- fc6
I0313 19:17:06.164551 12045 net.cpp:323] relu6 -> fc6 (in-place)
I0313 19:17:06.164577 12045 net.cpp:105] Setting up relu6
I0313 19:17:06.164597 12045 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:17:06.164609 12045 layer_factory.hpp:74] Creating layer drop6
I0313 19:17:06.164635 12045 net.cpp:76] Creating Layer drop6
I0313 19:17:06.164654 12045 net.cpp:372] drop6 <- fc6
I0313 19:17:06.164674 12045 net.cpp:323] drop6 -> fc6 (in-place)
I0313 19:17:06.164696 12045 net.cpp:105] Setting up drop6
I0313 19:17:06.164717 12045 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:17:06.164736 12045 layer_factory.hpp:74] Creating layer fc7
I0313 19:17:06.164760 12045 net.cpp:76] Creating Layer fc7
I0313 19:17:06.164779 12045 net.cpp:372] fc7 <- fc6
I0313 19:17:06.164803 12045 net.cpp:334] fc7 -> fc7
I0313 19:17:06.164830 12045 net.cpp:105] Setting up fc7
I0313 19:17:06.735321 12045 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:17:06.735412 12045 layer_factory.hpp:74] Creating layer relu7
I0313 19:17:06.735471 12045 net.cpp:76] Creating Layer relu7
I0313 19:17:06.735492 12045 net.cpp:372] relu7 <- fc7
I0313 19:17:06.735513 12045 net.cpp:323] relu7 -> fc7 (in-place)
I0313 19:17:06.735538 12045 net.cpp:105] Setting up relu7
I0313 19:17:06.735553 12045 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:17:06.735566 12045 layer_factory.hpp:74] Creating layer drop7
I0313 19:17:06.735586 12045 net.cpp:76] Creating Layer drop7
I0313 19:17:06.735601 12045 net.cpp:372] drop7 <- fc7
I0313 19:17:06.735637 12045 net.cpp:323] drop7 -> fc7 (in-place)
I0313 19:17:06.735687 12045 net.cpp:105] Setting up drop7
I0313 19:17:06.735704 12045 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:17:06.735719 12045 layer_factory.hpp:74] Creating layer fc8-alexnet11
I0313 19:17:06.735743 12045 net.cpp:76] Creating Layer fc8-alexnet11
I0313 19:17:06.735759 12045 net.cpp:372] fc8-alexnet11 <- fc7
I0313 19:17:06.735781 12045 net.cpp:334] fc8-alexnet11 -> fc8-alexnet11
I0313 19:17:06.735812 12045 net.cpp:105] Setting up fc8-alexnet11
I0313 19:17:06.737680 12045 net.cpp:112] Top shape: 50 11 1 1 (550)
I0313 19:17:06.737718 12045 layer_factory.hpp:74] Creating layer fc8-alexnet11_fc8-alexnet11_0_split
I0313 19:17:06.737747 12045 net.cpp:76] Creating Layer fc8-alexnet11_fc8-alexnet11_0_split
I0313 19:17:06.737767 12045 net.cpp:372] fc8-alexnet11_fc8-alexnet11_0_split <- fc8-alexnet11
I0313 19:17:06.737789 12045 net.cpp:334] fc8-alexnet11_fc8-alexnet11_0_split -> fc8-alexnet11_fc8-alexnet11_0_split_0
I0313 19:17:06.737812 12045 net.cpp:334] fc8-alexnet11_fc8-alexnet11_0_split -> fc8-alexnet11_fc8-alexnet11_0_split_1
I0313 19:17:06.737833 12045 net.cpp:105] Setting up fc8-alexnet11_fc8-alexnet11_0_split
I0313 19:17:06.737854 12045 net.cpp:112] Top shape: 50 11 1 1 (550)
I0313 19:17:06.737874 12045 net.cpp:112] Top shape: 50 11 1 1 (550)
I0313 19:17:06.737891 12045 layer_factory.hpp:74] Creating layer accuracy
I0313 19:17:06.737922 12045 net.cpp:76] Creating Layer accuracy
I0313 19:17:06.737949 12045 net.cpp:372] accuracy <- fc8-alexnet11_fc8-alexnet11_0_split_0
I0313 19:17:06.737969 12045 net.cpp:372] accuracy <- label_data_1_split_0
I0313 19:17:06.737996 12045 net.cpp:334] accuracy -> accuracy
I0313 19:17:06.738028 12045 net.cpp:105] Setting up accuracy
I0313 19:17:06.738052 12045 net.cpp:112] Top shape: 1 1 1 1 (1)
I0313 19:17:06.738077 12045 layer_factory.hpp:74] Creating layer loss
I0313 19:17:06.738101 12045 net.cpp:76] Creating Layer loss
I0313 19:17:06.738118 12045 net.cpp:372] loss <- fc8-alexnet11_fc8-alexnet11_0_split_1
I0313 19:17:06.738149 12045 net.cpp:372] loss <- label_data_1_split_1
I0313 19:17:06.738173 12045 net.cpp:334] loss -> loss
I0313 19:17:06.738205 12045 net.cpp:105] Setting up loss
I0313 19:17:06.738229 12045 layer_factory.hpp:74] Creating layer loss
I0313 19:17:06.738273 12045 net.cpp:112] Top shape: 1 1 1 1 (1)
I0313 19:17:06.738303 12045 net.cpp:118]     with loss weight 1
I0313 19:17:06.738354 12045 net.cpp:163] loss needs backward computation.
I0313 19:17:06.738373 12045 net.cpp:165] accuracy does not need backward computation.
I0313 19:17:06.738389 12045 net.cpp:163] fc8-alexnet11_fc8-alexnet11_0_split needs backward computation.
I0313 19:17:06.738436 12045 net.cpp:163] fc8-alexnet11 needs backward computation.
I0313 19:17:06.738461 12045 net.cpp:165] drop7 does not need backward computation.
I0313 19:17:06.738478 12045 net.cpp:165] relu7 does not need backward computation.
I0313 19:17:06.738492 12045 net.cpp:165] fc7 does not need backward computation.
I0313 19:17:06.738507 12045 net.cpp:165] drop6 does not need backward computation.
I0313 19:17:06.738523 12045 net.cpp:165] relu6 does not need backward computation.
I0313 19:17:06.738549 12045 net.cpp:165] fc6 does not need backward computation.
I0313 19:17:06.738567 12045 net.cpp:165] pool5 does not need backward computation.
I0313 19:17:06.738584 12045 net.cpp:165] relu5 does not need backward computation.
I0313 19:17:06.738600 12045 net.cpp:165] conv5 does not need backward computation.
I0313 19:17:06.738615 12045 net.cpp:165] relu4 does not need backward computation.
I0313 19:17:06.738629 12045 net.cpp:165] conv4 does not need backward computation.
I0313 19:17:06.738659 12045 net.cpp:165] relu3 does not need backward computation.
I0313 19:17:06.738678 12045 net.cpp:165] conv3 does not need backward computation.
I0313 19:17:06.738697 12045 net.cpp:165] norm2 does not need backward computation.
I0313 19:17:06.738713 12045 net.cpp:165] pool2 does not need backward computation.
I0313 19:17:06.738731 12045 net.cpp:165] relu2 does not need backward computation.
I0313 19:17:06.738766 12045 net.cpp:165] conv2 does not need backward computation.
I0313 19:17:06.738800 12045 net.cpp:165] norm1 does not need backward computation.
I0313 19:17:06.738831 12045 net.cpp:165] pool1 does not need backward computation.
I0313 19:17:06.738849 12045 net.cpp:165] relu1 does not need backward computation.
I0313 19:17:06.738867 12045 net.cpp:165] conv1 does not need backward computation.
I0313 19:17:06.738884 12045 net.cpp:165] label_data_1_split does not need backward computation.
I0313 19:17:06.738898 12045 net.cpp:165] data does not need backward computation.
I0313 19:17:06.738910 12045 net.cpp:201] This network produces output accuracy
I0313 19:17:06.738925 12045 net.cpp:201] This network produces output loss
I0313 19:17:06.738976 12045 net.cpp:446] Collecting Learning Rate and Weight Decay.
I0313 19:17:06.739007 12045 net.cpp:213] Network initialization done.
I0313 19:17:06.739025 12045 net.cpp:214] Memory required for data: 343014208
I0313 19:17:06.739177 12045 solver.cpp:42] Solver scaffolding done.
I0313 19:17:06.739238 12045 caffe.cpp:115] Finetuning from /root/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
E0313 19:17:08.404326 12045 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated transformation parameters: /root/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
I0313 19:17:08.404600 12045 upgrade_proto.cpp:612] Successfully upgraded file specified using deprecated data transformation parameters.
E0313 19:17:08.404618 12045 upgrade_proto.cpp:614] Note that future Caffe releases will only support transform_param messages for transformation fields.
E0313 19:17:08.404642 12045 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: /root/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
I0313 19:17:08.787623 12045 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0313 19:17:08.896167 12045 solver.cpp:222] Solving alexnet11
I0313 19:17:08.896250 12045 solver.cpp:223] Learning Rate Policy: step
I0313 19:17:08.896267 12045 solver.cpp:266] Iteration 0, Testing net (#0)
I0313 19:17:19.464882 12045 solver.cpp:315]     Test net output #0: accuracy = 0.0436667
I0313 19:17:19.464977 12045 solver.cpp:315]     Test net output #1: loss = 2.73736 (* 1 = 2.73736 loss)
I0313 19:17:19.782227 12045 solver.cpp:189] Iteration 0, loss = 2.92993
I0313 19:17:19.782333 12045 solver.cpp:204]     Train net output #0: loss = 2.92993 (* 1 = 2.92993 loss)
I0313 19:17:19.782364 12045 solver.cpp:470] Iteration 0, lr = 0.01
I0313 19:17:50.516639 12045 solver.cpp:189] Iteration 100, loss = 1.11161
I0313 19:17:50.516788 12045 solver.cpp:204]     Train net output #0: loss = 1.11161 (* 1 = 1.11161 loss)
I0313 19:17:50.516808 12045 solver.cpp:470] Iteration 100, lr = 0.01
I0313 19:18:21.406239 12045 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_200.caffemodel
I0313 19:18:22.345441 12045 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_200.solverstate
I0313 19:18:24.441818 12045 solver.cpp:266] Iteration 200, Testing net (#0)
I0313 19:18:34.838331 12045 solver.cpp:315]     Test net output #0: accuracy = 0.708667
I0313 19:18:34.838439 12045 solver.cpp:315]     Test net output #1: loss = 2.26661 (* 1 = 2.26661 loss)
I0313 19:18:35.131999 12045 solver.cpp:189] Iteration 200, loss = 0.916871
I0313 19:18:35.132077 12045 solver.cpp:204]     Train net output #0: loss = 0.916871 (* 1 = 0.916871 loss)
I0313 19:18:35.132097 12045 solver.cpp:470] Iteration 200, lr = 0.001
I0313 19:19:05.778131 12045 solver.cpp:189] Iteration 300, loss = 1.16717
I0313 19:19:05.779002 12045 solver.cpp:204]     Train net output #0: loss = 1.16717 (* 1 = 1.16717 loss)
I0313 19:19:05.779033 12045 solver.cpp:470] Iteration 300, lr = 0.001
I0313 19:19:36.567189 12045 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_400.caffemodel
I0313 19:19:37.753372 12045 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_400.solverstate
I0313 19:19:41.639716 12045 solver.cpp:266] Iteration 400, Testing net (#0)
I0313 19:19:52.101269 12045 solver.cpp:315]     Test net output #0: accuracy = 0.731
I0313 19:19:52.101385 12045 solver.cpp:315]     Test net output #1: loss = 2.10794 (* 1 = 2.10794 loss)
I0313 19:19:52.394297 12045 solver.cpp:189] Iteration 400, loss = 0.874007
I0313 19:19:52.394381 12045 solver.cpp:204]     Train net output #0: loss = 0.874007 (* 1 = 0.874007 loss)
I0313 19:19:52.394412 12045 solver.cpp:470] Iteration 400, lr = 0.0001
I0313 19:20:23.123857 12045 solver.cpp:189] Iteration 500, loss = 0.498559
I0313 19:20:23.124066 12045 solver.cpp:204]     Train net output #0: loss = 0.498559 (* 1 = 0.498559 loss)
I0313 19:20:23.124088 12045 solver.cpp:470] Iteration 500, lr = 0.0001
I0313 19:20:54.000035 12045 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_600.caffemodel
I0313 19:20:55.015897 12045 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_600.solverstate
I0313 19:20:58.927714 12045 solver.cpp:266] Iteration 600, Testing net (#0)
I0313 19:21:09.397620 12045 solver.cpp:315]     Test net output #0: accuracy = 0.727667
I0313 19:21:09.397727 12045 solver.cpp:315]     Test net output #1: loss = 2.07672 (* 1 = 2.07672 loss)
I0313 19:21:09.690691 12045 solver.cpp:189] Iteration 600, loss = 1.06672
I0313 19:21:09.690789 12045 solver.cpp:204]     Train net output #0: loss = 1.06672 (* 1 = 1.06672 loss)
I0313 19:21:09.690817 12045 solver.cpp:470] Iteration 600, lr = 1e-05
I0313 19:21:40.424403 12045 solver.cpp:189] Iteration 700, loss = 0.81397
I0313 19:21:40.424643 12045 solver.cpp:204]     Train net output #0: loss = 0.81397 (* 1 = 0.81397 loss)
I0313 19:21:40.424665 12045 solver.cpp:470] Iteration 700, lr = 1e-05
I0313 19:22:11.313796 12045 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_800.caffemodel
I0313 19:22:12.172950 12045 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_800.solverstate
I0313 19:22:15.519672 12045 solver.cpp:266] Iteration 800, Testing net (#0)
I0313 19:22:26.005205 12045 solver.cpp:315]     Test net output #0: accuracy = 0.724
I0313 19:22:26.005309 12045 solver.cpp:315]     Test net output #1: loss = 2.09809 (* 1 = 2.09809 loss)
I0313 19:22:26.298009 12045 solver.cpp:189] Iteration 800, loss = 0.605262
I0313 19:22:26.298107 12045 solver.cpp:204]     Train net output #0: loss = 0.605263 (* 1 = 0.605263 loss)
I0313 19:22:26.298128 12045 solver.cpp:470] Iteration 800, lr = 1e-06
I0313 19:22:57.027075 12045 solver.cpp:189] Iteration 900, loss = 1.03798
I0313 19:22:57.027315 12045 solver.cpp:204]     Train net output #0: loss = 1.03798 (* 1 = 1.03798 loss)
I0313 19:22:57.027338 12045 solver.cpp:470] Iteration 900, lr = 1e-06
I0313 19:23:27.907035 12045 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_1000.caffemodel
I0313 19:23:28.745079 12045 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_1000.solverstate
I0313 19:23:30.953135 12045 solver.cpp:266] Iteration 1000, Testing net (#0)
I0313 19:23:41.410666 12045 solver.cpp:315]     Test net output #0: accuracy = 0.724667
I0313 19:23:41.410783 12045 solver.cpp:315]     Test net output #1: loss = 2.05535 (* 1 = 2.05535 loss)
I0313 19:23:41.703656 12045 solver.cpp:189] Iteration 1000, loss = 1.18874
I0313 19:23:41.703735 12045 solver.cpp:204]     Train net output #0: loss = 1.18874 (* 1 = 1.18874 loss)
I0313 19:23:41.703754 12045 solver.cpp:470] Iteration 1000, lr = 1e-07
I0313 19:24:12.435694 12045 solver.cpp:189] Iteration 1100, loss = 0.590513
I0313 19:24:12.450362 12045 solver.cpp:204]     Train net output #0: loss = 0.590513 (* 1 = 0.590513 loss)
I0313 19:24:12.450408 12045 solver.cpp:470] Iteration 1100, lr = 1e-07
I0313 19:24:43.322916 12045 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_1200.caffemodel
I0313 19:24:45.147845 12045 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_1200.solverstate
I0313 19:24:49.151717 12045 solver.cpp:266] Iteration 1200, Testing net (#0)
I0313 19:24:59.640730 12045 solver.cpp:315]     Test net output #0: accuracy = 0.724
I0313 19:24:59.640841 12045 solver.cpp:315]     Test net output #1: loss = 2.08971 (* 1 = 2.08971 loss)
I0313 19:24:59.933727 12045 solver.cpp:189] Iteration 1200, loss = 0.981473
I0313 19:24:59.933814 12045 solver.cpp:204]     Train net output #0: loss = 0.981474 (* 1 = 0.981474 loss)
I0313 19:24:59.933842 12045 solver.cpp:470] Iteration 1200, lr = 1e-08
I0313 19:25:30.669244 12045 solver.cpp:189] Iteration 1300, loss = 0.607853
I0313 19:25:30.669494 12045 solver.cpp:204]     Train net output #0: loss = 0.607854 (* 1 = 0.607854 loss)
I0313 19:25:30.669534 12045 solver.cpp:470] Iteration 1300, lr = 1e-08
I0313 19:26:01.558293 12045 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_1400.caffemodel
I0313 19:26:02.403154 12045 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/fix_all_other_layers/results_exp1/snapshots/final/alexnet11_fix_all_other_layers_iter_1400.solverstate
I0313 19:26:04.532759 12045 solver.cpp:266] Iteration 1400, Testing net (#0)
I0313 19:26:15.002140 12045 solver.cpp:315]     Test net output #0: accuracy = 0.725
I0313 19:26:15.002249 12045 solver.cpp:315]     Test net output #1: loss = 2.07938 (* 1 = 2.07938 loss)
I0313 19:26:15.295163 12045 solver.cpp:189] Iteration 1400, loss = 0.784544
I0313 19:26:15.295258 12045 solver.cpp:204]     Train net output #0: loss = 0.784545 (* 1 = 0.784545 loss)
I0313 19:26:15.295287 12045 solver.cpp:470] Iteration 1400, lr = 1e-09
