I0313 19:38:57.814849 14018 caffe.cpp:99] Use GPU with device ID 0
I0313 19:38:58.173339 14018 caffe.cpp:107] Starting Optimization
I0313 19:38:58.173527 14018 solver.cpp:32] Initializing solver from parameters: 
test_iter: 60
test_interval: 200
base_lr: 0.0001
display: 100
max_iter: 2000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 200
snapshot: 200
snapshot_prefix: "/root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers"
solver_mode: GPU
net: "/root/cs231n-project/cnns/alexnet-11/train_all_layers/train_val.prototxt"
I0313 19:38:58.173574 14018 solver.cpp:70] Creating training net from net file: /root/cs231n-project/cnns/alexnet-11/train_all_layers/train_val.prototxt
I0313 19:38:58.175189 14018 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0313 19:38:58.175238 14018 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0313 19:38:58.175461 14018 net.cpp:42] Initializing net from parameters: 
name: "alexnet11"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/root/cs231n-project/data/images/train/no_augmentations/imagenet/256/train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-alexnet11"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-alexnet11"
  param {
    lr_mult: 10
    decay_mult: 10
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-alexnet11"
  bottom: "label"
  top: "loss"
}
I0313 19:38:58.175621 14018 layer_factory.hpp:74] Creating layer data
I0313 19:38:58.175658 14018 net.cpp:76] Creating Layer data
I0313 19:38:58.175675 14018 net.cpp:334] data -> data
I0313 19:38:58.175712 14018 net.cpp:334] data -> label
I0313 19:38:58.175737 14018 net.cpp:105] Setting up data
I0313 19:38:58.175853 14018 db.cpp:34] Opened lmdb /root/cs231n-project/data/images/train/no_augmentations/imagenet/256/train_lmdb
I0313 19:38:58.176182 14018 data_layer.cpp:67] output data size: 100,3,227,227
I0313 19:38:58.176213 14018 data_transformer.cpp:22] Loading mean file from: /root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto
I0313 19:38:58.207161 14018 net.cpp:112] Top shape: 100 3 227 227 (15458700)
I0313 19:38:58.207237 14018 net.cpp:112] Top shape: 100 1 1 1 (100)
I0313 19:38:58.207253 14018 layer_factory.hpp:74] Creating layer conv1
I0313 19:38:58.207293 14018 net.cpp:76] Creating Layer conv1
I0313 19:38:58.207306 14018 net.cpp:372] conv1 <- data
I0313 19:38:58.207332 14018 net.cpp:334] conv1 -> conv1
I0313 19:38:58.207356 14018 net.cpp:105] Setting up conv1
I0313 19:38:58.208984 14018 net.cpp:112] Top shape: 100 96 55 55 (29040000)
I0313 19:38:58.209036 14018 layer_factory.hpp:74] Creating layer relu1
I0313 19:38:58.209053 14018 net.cpp:76] Creating Layer relu1
I0313 19:38:58.209061 14018 net.cpp:372] relu1 <- conv1
I0313 19:38:58.209070 14018 net.cpp:323] relu1 -> conv1 (in-place)
I0313 19:38:58.209089 14018 net.cpp:105] Setting up relu1
I0313 19:38:58.209100 14018 net.cpp:112] Top shape: 100 96 55 55 (29040000)
I0313 19:38:58.209112 14018 layer_factory.hpp:74] Creating layer pool1
I0313 19:38:58.209125 14018 net.cpp:76] Creating Layer pool1
I0313 19:38:58.209137 14018 net.cpp:372] pool1 <- conv1
I0313 19:38:58.209147 14018 net.cpp:334] pool1 -> pool1
I0313 19:38:58.209167 14018 net.cpp:105] Setting up pool1
I0313 19:38:58.209276 14018 net.cpp:112] Top shape: 100 96 27 27 (6998400)
I0313 19:38:58.209302 14018 layer_factory.hpp:74] Creating layer norm1
I0313 19:38:58.209372 14018 net.cpp:76] Creating Layer norm1
I0313 19:38:58.209396 14018 net.cpp:372] norm1 <- pool1
I0313 19:38:58.209424 14018 net.cpp:334] norm1 -> norm1
I0313 19:38:58.209470 14018 net.cpp:105] Setting up norm1
I0313 19:38:58.209569 14018 net.cpp:112] Top shape: 100 96 27 27 (6998400)
I0313 19:38:58.209589 14018 layer_factory.hpp:74] Creating layer conv2
I0313 19:38:58.209609 14018 net.cpp:76] Creating Layer conv2
I0313 19:38:58.209619 14018 net.cpp:372] conv2 <- norm1
I0313 19:38:58.209635 14018 net.cpp:334] conv2 -> conv2
I0313 19:38:58.209650 14018 net.cpp:105] Setting up conv2
I0313 19:38:58.222453 14018 net.cpp:112] Top shape: 100 256 27 27 (18662400)
I0313 19:38:58.222502 14018 layer_factory.hpp:74] Creating layer relu2
I0313 19:38:58.222522 14018 net.cpp:76] Creating Layer relu2
I0313 19:38:58.222532 14018 net.cpp:372] relu2 <- conv2
I0313 19:38:58.222542 14018 net.cpp:323] relu2 -> conv2 (in-place)
I0313 19:38:58.222551 14018 net.cpp:105] Setting up relu2
I0313 19:38:58.222558 14018 net.cpp:112] Top shape: 100 256 27 27 (18662400)
I0313 19:38:58.222566 14018 layer_factory.hpp:74] Creating layer pool2
I0313 19:38:58.222575 14018 net.cpp:76] Creating Layer pool2
I0313 19:38:58.222582 14018 net.cpp:372] pool2 <- conv2
I0313 19:38:58.222594 14018 net.cpp:334] pool2 -> pool2
I0313 19:38:58.222604 14018 net.cpp:105] Setting up pool2
I0313 19:38:58.222614 14018 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:38:58.222621 14018 layer_factory.hpp:74] Creating layer norm2
I0313 19:38:58.222633 14018 net.cpp:76] Creating Layer norm2
I0313 19:38:58.222642 14018 net.cpp:372] norm2 <- pool2
I0313 19:38:58.222654 14018 net.cpp:334] norm2 -> norm2
I0313 19:38:58.222666 14018 net.cpp:105] Setting up norm2
I0313 19:38:58.222676 14018 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:38:58.222683 14018 layer_factory.hpp:74] Creating layer conv3
I0313 19:38:58.222702 14018 net.cpp:76] Creating Layer conv3
I0313 19:38:58.222724 14018 net.cpp:372] conv3 <- norm2
I0313 19:38:58.222739 14018 net.cpp:334] conv3 -> conv3
I0313 19:38:58.222753 14018 net.cpp:105] Setting up conv3
I0313 19:38:58.256343 14018 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:38:58.256445 14018 layer_factory.hpp:74] Creating layer relu3
I0313 19:38:58.256466 14018 net.cpp:76] Creating Layer relu3
I0313 19:38:58.256479 14018 net.cpp:372] relu3 <- conv3
I0313 19:38:58.256489 14018 net.cpp:323] relu3 -> conv3 (in-place)
I0313 19:38:58.256502 14018 net.cpp:105] Setting up relu3
I0313 19:38:58.256510 14018 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:38:58.256517 14018 layer_factory.hpp:74] Creating layer conv4
I0313 19:38:58.256533 14018 net.cpp:76] Creating Layer conv4
I0313 19:38:58.256544 14018 net.cpp:372] conv4 <- conv3
I0313 19:38:58.256553 14018 net.cpp:334] conv4 -> conv4
I0313 19:38:58.256566 14018 net.cpp:105] Setting up conv4
I0313 19:38:58.282008 14018 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:38:58.282073 14018 layer_factory.hpp:74] Creating layer relu4
I0313 19:38:58.282097 14018 net.cpp:76] Creating Layer relu4
I0313 19:38:58.282107 14018 net.cpp:372] relu4 <- conv4
I0313 19:38:58.282126 14018 net.cpp:323] relu4 -> conv4 (in-place)
I0313 19:38:58.282142 14018 net.cpp:105] Setting up relu4
I0313 19:38:58.282176 14018 net.cpp:112] Top shape: 100 384 13 13 (6489600)
I0313 19:38:58.282186 14018 layer_factory.hpp:74] Creating layer conv5
I0313 19:38:58.282203 14018 net.cpp:76] Creating Layer conv5
I0313 19:38:58.282212 14018 net.cpp:372] conv5 <- conv4
I0313 19:38:58.282222 14018 net.cpp:334] conv5 -> conv5
I0313 19:38:58.282234 14018 net.cpp:105] Setting up conv5
I0313 19:38:58.298957 14018 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:38:58.299021 14018 layer_factory.hpp:74] Creating layer relu5
I0313 19:38:58.299037 14018 net.cpp:76] Creating Layer relu5
I0313 19:38:58.299046 14018 net.cpp:372] relu5 <- conv5
I0313 19:38:58.299057 14018 net.cpp:323] relu5 -> conv5 (in-place)
I0313 19:38:58.299068 14018 net.cpp:105] Setting up relu5
I0313 19:38:58.299075 14018 net.cpp:112] Top shape: 100 256 13 13 (4326400)
I0313 19:38:58.299083 14018 layer_factory.hpp:74] Creating layer pool5
I0313 19:38:58.299105 14018 net.cpp:76] Creating Layer pool5
I0313 19:38:58.299135 14018 net.cpp:372] pool5 <- conv5
I0313 19:38:58.299150 14018 net.cpp:334] pool5 -> pool5
I0313 19:38:58.299175 14018 net.cpp:105] Setting up pool5
I0313 19:38:58.299187 14018 net.cpp:112] Top shape: 100 256 6 6 (921600)
I0313 19:38:58.299209 14018 layer_factory.hpp:74] Creating layer fc6
I0313 19:38:58.299232 14018 net.cpp:76] Creating Layer fc6
I0313 19:38:58.299247 14018 net.cpp:372] fc6 <- pool5
I0313 19:38:58.299258 14018 net.cpp:334] fc6 -> fc6
I0313 19:38:58.299275 14018 net.cpp:105] Setting up fc6
I0313 19:38:59.567919 14018 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:38:59.567994 14018 layer_factory.hpp:74] Creating layer relu6
I0313 19:38:59.568017 14018 net.cpp:76] Creating Layer relu6
I0313 19:38:59.568027 14018 net.cpp:372] relu6 <- fc6
I0313 19:38:59.568042 14018 net.cpp:323] relu6 -> fc6 (in-place)
I0313 19:38:59.568054 14018 net.cpp:105] Setting up relu6
I0313 19:38:59.568063 14018 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:38:59.568070 14018 layer_factory.hpp:74] Creating layer drop6
I0313 19:38:59.568197 14018 net.cpp:76] Creating Layer drop6
I0313 19:38:59.568210 14018 net.cpp:372] drop6 <- fc6
I0313 19:38:59.568223 14018 net.cpp:323] drop6 -> fc6 (in-place)
I0313 19:38:59.568240 14018 net.cpp:105] Setting up drop6
I0313 19:38:59.568253 14018 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:38:59.568265 14018 layer_factory.hpp:74] Creating layer fc7
I0313 19:38:59.568279 14018 net.cpp:76] Creating Layer fc7
I0313 19:38:59.568286 14018 net.cpp:372] fc7 <- fc6
I0313 19:38:59.568300 14018 net.cpp:334] fc7 -> fc7
I0313 19:38:59.568313 14018 net.cpp:105] Setting up fc7
I0313 19:39:00.132904 14018 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:39:00.133003 14018 layer_factory.hpp:74] Creating layer relu7
I0313 19:39:00.133025 14018 net.cpp:76] Creating Layer relu7
I0313 19:39:00.133035 14018 net.cpp:372] relu7 <- fc7
I0313 19:39:00.133051 14018 net.cpp:323] relu7 -> fc7 (in-place)
I0313 19:39:00.133065 14018 net.cpp:105] Setting up relu7
I0313 19:39:00.133074 14018 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:39:00.133081 14018 layer_factory.hpp:74] Creating layer drop7
I0313 19:39:00.133092 14018 net.cpp:76] Creating Layer drop7
I0313 19:39:00.133100 14018 net.cpp:372] drop7 <- fc7
I0313 19:39:00.133107 14018 net.cpp:323] drop7 -> fc7 (in-place)
I0313 19:39:00.133116 14018 net.cpp:105] Setting up drop7
I0313 19:39:00.133126 14018 net.cpp:112] Top shape: 100 4096 1 1 (409600)
I0313 19:39:00.133132 14018 layer_factory.hpp:74] Creating layer fc8-alexnet11
I0313 19:39:00.133147 14018 net.cpp:76] Creating Layer fc8-alexnet11
I0313 19:39:00.133154 14018 net.cpp:372] fc8-alexnet11 <- fc7
I0313 19:39:00.133163 14018 net.cpp:334] fc8-alexnet11 -> fc8-alexnet11
I0313 19:39:00.133177 14018 net.cpp:105] Setting up fc8-alexnet11
I0313 19:39:00.134743 14018 net.cpp:112] Top shape: 100 11 1 1 (1100)
I0313 19:39:00.134773 14018 layer_factory.hpp:74] Creating layer loss
I0313 19:39:00.134791 14018 net.cpp:76] Creating Layer loss
I0313 19:39:00.134802 14018 net.cpp:372] loss <- fc8-alexnet11
I0313 19:39:00.134814 14018 net.cpp:372] loss <- label
I0313 19:39:00.134827 14018 net.cpp:334] loss -> loss
I0313 19:39:00.134847 14018 net.cpp:105] Setting up loss
I0313 19:39:00.134868 14018 layer_factory.hpp:74] Creating layer loss
I0313 19:39:00.134903 14018 net.cpp:112] Top shape: 1 1 1 1 (1)
I0313 19:39:00.134920 14018 net.cpp:118]     with loss weight 1
I0313 19:39:00.134968 14018 net.cpp:163] loss needs backward computation.
I0313 19:39:00.134976 14018 net.cpp:163] fc8-alexnet11 needs backward computation.
I0313 19:39:00.134992 14018 net.cpp:163] drop7 needs backward computation.
I0313 19:39:00.135000 14018 net.cpp:163] relu7 needs backward computation.
I0313 19:39:00.135006 14018 net.cpp:163] fc7 needs backward computation.
I0313 19:39:00.135020 14018 net.cpp:163] drop6 needs backward computation.
I0313 19:39:00.135028 14018 net.cpp:163] relu6 needs backward computation.
I0313 19:39:00.135048 14018 net.cpp:163] fc6 needs backward computation.
I0313 19:39:00.135078 14018 net.cpp:163] pool5 needs backward computation.
I0313 19:39:00.135087 14018 net.cpp:163] relu5 needs backward computation.
I0313 19:39:00.135099 14018 net.cpp:163] conv5 needs backward computation.
I0313 19:39:00.135107 14018 net.cpp:163] relu4 needs backward computation.
I0313 19:39:00.135113 14018 net.cpp:163] conv4 needs backward computation.
I0313 19:39:00.135126 14018 net.cpp:163] relu3 needs backward computation.
I0313 19:39:00.135134 14018 net.cpp:163] conv3 needs backward computation.
I0313 19:39:00.135141 14018 net.cpp:163] norm2 needs backward computation.
I0313 19:39:00.135149 14018 net.cpp:163] pool2 needs backward computation.
I0313 19:39:00.135155 14018 net.cpp:163] relu2 needs backward computation.
I0313 19:39:00.135161 14018 net.cpp:163] conv2 needs backward computation.
I0313 19:39:00.135169 14018 net.cpp:163] norm1 needs backward computation.
I0313 19:39:00.135180 14018 net.cpp:163] pool1 needs backward computation.
I0313 19:39:00.135188 14018 net.cpp:163] relu1 needs backward computation.
I0313 19:39:00.135195 14018 net.cpp:163] conv1 needs backward computation.
I0313 19:39:00.135211 14018 net.cpp:165] data does not need backward computation.
I0313 19:39:00.135220 14018 net.cpp:201] This network produces output loss
I0313 19:39:00.135239 14018 net.cpp:446] Collecting Learning Rate and Weight Decay.
I0313 19:39:00.135259 14018 net.cpp:213] Network initialization done.
I0313 19:39:00.135272 14018 net.cpp:214] Memory required for data: 686018804
I0313 19:39:00.135915 14018 solver.cpp:154] Creating test net (#0) specified by net file: /root/cs231n-project/cnns/alexnet-11/train_all_layers/train_val.prototxt
I0313 19:39:00.135977 14018 net.cpp:253] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0313 19:39:00.136189 14018 net.cpp:42] Initializing net from parameters: 
name: "alexnet11"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/root/cs231n-project/data/images/val/instagram/227/val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-alexnet11"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-alexnet11"
  param {
    lr_mult: 10
    decay_mult: 10
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8-alexnet11"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-alexnet11"
  bottom: "label"
  top: "loss"
}
I0313 19:39:00.136339 14018 layer_factory.hpp:74] Creating layer data
I0313 19:39:00.136364 14018 net.cpp:76] Creating Layer data
I0313 19:39:00.136375 14018 net.cpp:334] data -> data
I0313 19:39:00.136394 14018 net.cpp:334] data -> label
I0313 19:39:00.136407 14018 net.cpp:105] Setting up data
I0313 19:39:00.136497 14018 db.cpp:34] Opened lmdb /root/cs231n-project/data/images/val/instagram/227/val_lmdb
I0313 19:39:00.136795 14018 data_layer.cpp:67] output data size: 50,3,227,227
I0313 19:39:00.136821 14018 data_transformer.cpp:22] Loading mean file from: /root/cs231n-project/data/image_means/ilsvrc12/imagenet_mean.binaryproto
I0313 19:39:00.152957 14018 net.cpp:112] Top shape: 50 3 227 227 (7729350)
I0313 19:39:00.153028 14018 net.cpp:112] Top shape: 50 1 1 1 (50)
I0313 19:39:00.153044 14018 layer_factory.hpp:74] Creating layer label_data_1_split
I0313 19:39:00.153065 14018 net.cpp:76] Creating Layer label_data_1_split
I0313 19:39:00.153075 14018 net.cpp:372] label_data_1_split <- label
I0313 19:39:00.153089 14018 net.cpp:334] label_data_1_split -> label_data_1_split_0
I0313 19:39:00.153106 14018 net.cpp:334] label_data_1_split -> label_data_1_split_1
I0313 19:39:00.153146 14018 net.cpp:105] Setting up label_data_1_split
I0313 19:39:00.153164 14018 net.cpp:112] Top shape: 50 1 1 1 (50)
I0313 19:39:00.153177 14018 net.cpp:112] Top shape: 50 1 1 1 (50)
I0313 19:39:00.153185 14018 layer_factory.hpp:74] Creating layer conv1
I0313 19:39:00.153203 14018 net.cpp:76] Creating Layer conv1
I0313 19:39:00.153214 14018 net.cpp:372] conv1 <- data
I0313 19:39:00.153226 14018 net.cpp:334] conv1 -> conv1
I0313 19:39:00.153239 14018 net.cpp:105] Setting up conv1
I0313 19:39:00.154410 14018 net.cpp:112] Top shape: 50 96 55 55 (14520000)
I0313 19:39:00.154445 14018 layer_factory.hpp:74] Creating layer relu1
I0313 19:39:00.154458 14018 net.cpp:76] Creating Layer relu1
I0313 19:39:00.154484 14018 net.cpp:372] relu1 <- conv1
I0313 19:39:00.154497 14018 net.cpp:323] relu1 -> conv1 (in-place)
I0313 19:39:00.154513 14018 net.cpp:105] Setting up relu1
I0313 19:39:00.154522 14018 net.cpp:112] Top shape: 50 96 55 55 (14520000)
I0313 19:39:00.154531 14018 layer_factory.hpp:74] Creating layer pool1
I0313 19:39:00.154551 14018 net.cpp:76] Creating Layer pool1
I0313 19:39:00.154564 14018 net.cpp:372] pool1 <- conv1
I0313 19:39:00.154574 14018 net.cpp:334] pool1 -> pool1
I0313 19:39:00.154595 14018 net.cpp:105] Setting up pool1
I0313 19:39:00.154619 14018 net.cpp:112] Top shape: 50 96 27 27 (3499200)
I0313 19:39:00.154635 14018 layer_factory.hpp:74] Creating layer norm1
I0313 19:39:00.154649 14018 net.cpp:76] Creating Layer norm1
I0313 19:39:00.154661 14018 net.cpp:372] norm1 <- pool1
I0313 19:39:00.154672 14018 net.cpp:334] norm1 -> norm1
I0313 19:39:00.154688 14018 net.cpp:105] Setting up norm1
I0313 19:39:00.154698 14018 net.cpp:112] Top shape: 50 96 27 27 (3499200)
I0313 19:39:00.154711 14018 layer_factory.hpp:74] Creating layer conv2
I0313 19:39:00.154722 14018 net.cpp:76] Creating Layer conv2
I0313 19:39:00.154736 14018 net.cpp:372] conv2 <- norm1
I0313 19:39:00.154754 14018 net.cpp:334] conv2 -> conv2
I0313 19:39:00.154772 14018 net.cpp:105] Setting up conv2
I0313 19:39:00.164890 14018 net.cpp:112] Top shape: 50 256 27 27 (9331200)
I0313 19:39:00.164934 14018 layer_factory.hpp:74] Creating layer relu2
I0313 19:39:00.164948 14018 net.cpp:76] Creating Layer relu2
I0313 19:39:00.164957 14018 net.cpp:372] relu2 <- conv2
I0313 19:39:00.164965 14018 net.cpp:323] relu2 -> conv2 (in-place)
I0313 19:39:00.164975 14018 net.cpp:105] Setting up relu2
I0313 19:39:00.164983 14018 net.cpp:112] Top shape: 50 256 27 27 (9331200)
I0313 19:39:00.164989 14018 layer_factory.hpp:74] Creating layer pool2
I0313 19:39:00.165001 14018 net.cpp:76] Creating Layer pool2
I0313 19:39:00.165007 14018 net.cpp:372] pool2 <- conv2
I0313 19:39:00.165019 14018 net.cpp:334] pool2 -> pool2
I0313 19:39:00.165033 14018 net.cpp:105] Setting up pool2
I0313 19:39:00.165043 14018 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:39:00.165052 14018 layer_factory.hpp:74] Creating layer norm2
I0313 19:39:00.165067 14018 net.cpp:76] Creating Layer norm2
I0313 19:39:00.165087 14018 net.cpp:372] norm2 <- pool2
I0313 19:39:00.165098 14018 net.cpp:334] norm2 -> norm2
I0313 19:39:00.165108 14018 net.cpp:105] Setting up norm2
I0313 19:39:00.165127 14018 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:39:00.165137 14018 layer_factory.hpp:74] Creating layer conv3
I0313 19:39:00.165148 14018 net.cpp:76] Creating Layer conv3
I0313 19:39:00.165163 14018 net.cpp:372] conv3 <- norm2
I0313 19:39:00.165176 14018 net.cpp:334] conv3 -> conv3
I0313 19:39:00.165197 14018 net.cpp:105] Setting up conv3
I0313 19:39:00.195016 14018 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:39:00.195101 14018 layer_factory.hpp:74] Creating layer relu3
I0313 19:39:00.195122 14018 net.cpp:76] Creating Layer relu3
I0313 19:39:00.195132 14018 net.cpp:372] relu3 <- conv3
I0313 19:39:00.195144 14018 net.cpp:323] relu3 -> conv3 (in-place)
I0313 19:39:00.195158 14018 net.cpp:105] Setting up relu3
I0313 19:39:00.195164 14018 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:39:00.195171 14018 layer_factory.hpp:74] Creating layer conv4
I0313 19:39:00.195197 14018 net.cpp:76] Creating Layer conv4
I0313 19:39:00.195224 14018 net.cpp:372] conv4 <- conv3
I0313 19:39:00.195235 14018 net.cpp:334] conv4 -> conv4
I0313 19:39:00.195246 14018 net.cpp:105] Setting up conv4
I0313 19:39:00.217525 14018 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:39:00.217563 14018 layer_factory.hpp:74] Creating layer relu4
I0313 19:39:00.217578 14018 net.cpp:76] Creating Layer relu4
I0313 19:39:00.217586 14018 net.cpp:372] relu4 <- conv4
I0313 19:39:00.217594 14018 net.cpp:323] relu4 -> conv4 (in-place)
I0313 19:39:00.217604 14018 net.cpp:105] Setting up relu4
I0313 19:39:00.217625 14018 net.cpp:112] Top shape: 50 384 13 13 (3244800)
I0313 19:39:00.217635 14018 layer_factory.hpp:74] Creating layer conv5
I0313 19:39:00.217649 14018 net.cpp:76] Creating Layer conv5
I0313 19:39:00.217658 14018 net.cpp:372] conv5 <- conv4
I0313 19:39:00.217666 14018 net.cpp:334] conv5 -> conv5
I0313 19:39:00.217679 14018 net.cpp:105] Setting up conv5
I0313 19:39:00.232517 14018 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:39:00.232568 14018 layer_factory.hpp:74] Creating layer relu5
I0313 19:39:00.232585 14018 net.cpp:76] Creating Layer relu5
I0313 19:39:00.232594 14018 net.cpp:372] relu5 <- conv5
I0313 19:39:00.232604 14018 net.cpp:323] relu5 -> conv5 (in-place)
I0313 19:39:00.232616 14018 net.cpp:105] Setting up relu5
I0313 19:39:00.232624 14018 net.cpp:112] Top shape: 50 256 13 13 (2163200)
I0313 19:39:00.232630 14018 layer_factory.hpp:74] Creating layer pool5
I0313 19:39:00.232645 14018 net.cpp:76] Creating Layer pool5
I0313 19:39:00.232661 14018 net.cpp:372] pool5 <- conv5
I0313 19:39:00.232671 14018 net.cpp:334] pool5 -> pool5
I0313 19:39:00.232681 14018 net.cpp:105] Setting up pool5
I0313 19:39:00.232702 14018 net.cpp:112] Top shape: 50 256 6 6 (460800)
I0313 19:39:00.232710 14018 layer_factory.hpp:74] Creating layer fc6
I0313 19:39:00.232723 14018 net.cpp:76] Creating Layer fc6
I0313 19:39:00.232731 14018 net.cpp:372] fc6 <- pool5
I0313 19:39:00.232743 14018 net.cpp:334] fc6 -> fc6
I0313 19:39:00.232755 14018 net.cpp:105] Setting up fc6
I0313 19:39:01.500215 14018 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:39:01.500303 14018 layer_factory.hpp:74] Creating layer relu6
I0313 19:39:01.500324 14018 net.cpp:76] Creating Layer relu6
I0313 19:39:01.500334 14018 net.cpp:372] relu6 <- fc6
I0313 19:39:01.500346 14018 net.cpp:323] relu6 -> fc6 (in-place)
I0313 19:39:01.500360 14018 net.cpp:105] Setting up relu6
I0313 19:39:01.500367 14018 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:39:01.500375 14018 layer_factory.hpp:74] Creating layer drop6
I0313 19:39:01.500386 14018 net.cpp:76] Creating Layer drop6
I0313 19:39:01.500394 14018 net.cpp:372] drop6 <- fc6
I0313 19:39:01.500404 14018 net.cpp:323] drop6 -> fc6 (in-place)
I0313 19:39:01.500417 14018 net.cpp:105] Setting up drop6
I0313 19:39:01.500432 14018 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:39:01.500447 14018 layer_factory.hpp:74] Creating layer fc7
I0313 19:39:01.500488 14018 net.cpp:76] Creating Layer fc7
I0313 19:39:01.500500 14018 net.cpp:372] fc7 <- fc6
I0313 19:39:01.500511 14018 net.cpp:334] fc7 -> fc7
I0313 19:39:01.500524 14018 net.cpp:105] Setting up fc7
I0313 19:39:02.068140 14018 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:39:02.068220 14018 layer_factory.hpp:74] Creating layer relu7
I0313 19:39:02.068241 14018 net.cpp:76] Creating Layer relu7
I0313 19:39:02.068251 14018 net.cpp:372] relu7 <- fc7
I0313 19:39:02.068264 14018 net.cpp:323] relu7 -> fc7 (in-place)
I0313 19:39:02.068277 14018 net.cpp:105] Setting up relu7
I0313 19:39:02.068284 14018 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:39:02.068291 14018 layer_factory.hpp:74] Creating layer drop7
I0313 19:39:02.068305 14018 net.cpp:76] Creating Layer drop7
I0313 19:39:02.068313 14018 net.cpp:372] drop7 <- fc7
I0313 19:39:02.068321 14018 net.cpp:323] drop7 -> fc7 (in-place)
I0313 19:39:02.068332 14018 net.cpp:105] Setting up drop7
I0313 19:39:02.068341 14018 net.cpp:112] Top shape: 50 4096 1 1 (204800)
I0313 19:39:02.068358 14018 layer_factory.hpp:74] Creating layer fc8-alexnet11
I0313 19:39:02.068389 14018 net.cpp:76] Creating Layer fc8-alexnet11
I0313 19:39:02.068397 14018 net.cpp:372] fc8-alexnet11 <- fc7
I0313 19:39:02.068406 14018 net.cpp:334] fc8-alexnet11 -> fc8-alexnet11
I0313 19:39:02.068423 14018 net.cpp:105] Setting up fc8-alexnet11
I0313 19:39:02.069998 14018 net.cpp:112] Top shape: 50 11 1 1 (550)
I0313 19:39:02.070027 14018 layer_factory.hpp:74] Creating layer fc8-alexnet11_fc8-alexnet11_0_split
I0313 19:39:02.070041 14018 net.cpp:76] Creating Layer fc8-alexnet11_fc8-alexnet11_0_split
I0313 19:39:02.070050 14018 net.cpp:372] fc8-alexnet11_fc8-alexnet11_0_split <- fc8-alexnet11
I0313 19:39:02.070061 14018 net.cpp:334] fc8-alexnet11_fc8-alexnet11_0_split -> fc8-alexnet11_fc8-alexnet11_0_split_0
I0313 19:39:02.070085 14018 net.cpp:334] fc8-alexnet11_fc8-alexnet11_0_split -> fc8-alexnet11_fc8-alexnet11_0_split_1
I0313 19:39:02.070098 14018 net.cpp:105] Setting up fc8-alexnet11_fc8-alexnet11_0_split
I0313 19:39:02.070109 14018 net.cpp:112] Top shape: 50 11 1 1 (550)
I0313 19:39:02.070122 14018 net.cpp:112] Top shape: 50 11 1 1 (550)
I0313 19:39:02.070130 14018 layer_factory.hpp:74] Creating layer accuracy
I0313 19:39:02.070148 14018 net.cpp:76] Creating Layer accuracy
I0313 19:39:02.070161 14018 net.cpp:372] accuracy <- fc8-alexnet11_fc8-alexnet11_0_split_0
I0313 19:39:02.070170 14018 net.cpp:372] accuracy <- label_data_1_split_0
I0313 19:39:02.070180 14018 net.cpp:334] accuracy -> accuracy
I0313 19:39:02.070195 14018 net.cpp:105] Setting up accuracy
I0313 19:39:02.070206 14018 net.cpp:112] Top shape: 1 1 1 1 (1)
I0313 19:39:02.070219 14018 layer_factory.hpp:74] Creating layer loss
I0313 19:39:02.070230 14018 net.cpp:76] Creating Layer loss
I0313 19:39:02.070242 14018 net.cpp:372] loss <- fc8-alexnet11_fc8-alexnet11_0_split_1
I0313 19:39:02.070251 14018 net.cpp:372] loss <- label_data_1_split_1
I0313 19:39:02.070266 14018 net.cpp:334] loss -> loss
I0313 19:39:02.070282 14018 net.cpp:105] Setting up loss
I0313 19:39:02.070303 14018 layer_factory.hpp:74] Creating layer loss
I0313 19:39:02.070324 14018 net.cpp:112] Top shape: 1 1 1 1 (1)
I0313 19:39:02.070338 14018 net.cpp:118]     with loss weight 1
I0313 19:39:02.070366 14018 net.cpp:163] loss needs backward computation.
I0313 19:39:02.070374 14018 net.cpp:165] accuracy does not need backward computation.
I0313 19:39:02.070381 14018 net.cpp:163] fc8-alexnet11_fc8-alexnet11_0_split needs backward computation.
I0313 19:39:02.070389 14018 net.cpp:163] fc8-alexnet11 needs backward computation.
I0313 19:39:02.070394 14018 net.cpp:163] drop7 needs backward computation.
I0313 19:39:02.070400 14018 net.cpp:163] relu7 needs backward computation.
I0313 19:39:02.070406 14018 net.cpp:163] fc7 needs backward computation.
I0313 19:39:02.070413 14018 net.cpp:163] drop6 needs backward computation.
I0313 19:39:02.070428 14018 net.cpp:163] relu6 needs backward computation.
I0313 19:39:02.070458 14018 net.cpp:163] fc6 needs backward computation.
I0313 19:39:02.070466 14018 net.cpp:163] pool5 needs backward computation.
I0313 19:39:02.070473 14018 net.cpp:163] relu5 needs backward computation.
I0313 19:39:02.070487 14018 net.cpp:163] conv5 needs backward computation.
I0313 19:39:02.070497 14018 net.cpp:163] relu4 needs backward computation.
I0313 19:39:02.070509 14018 net.cpp:163] conv4 needs backward computation.
I0313 19:39:02.070518 14018 net.cpp:163] relu3 needs backward computation.
I0313 19:39:02.070524 14018 net.cpp:163] conv3 needs backward computation.
I0313 19:39:02.070536 14018 net.cpp:163] norm2 needs backward computation.
I0313 19:39:02.070544 14018 net.cpp:163] pool2 needs backward computation.
I0313 19:39:02.070551 14018 net.cpp:163] relu2 needs backward computation.
I0313 19:39:02.070564 14018 net.cpp:163] conv2 needs backward computation.
I0313 19:39:02.070572 14018 net.cpp:163] norm1 needs backward computation.
I0313 19:39:02.070580 14018 net.cpp:163] pool1 needs backward computation.
I0313 19:39:02.070587 14018 net.cpp:163] relu1 needs backward computation.
I0313 19:39:02.070608 14018 net.cpp:163] conv1 needs backward computation.
I0313 19:39:02.070638 14018 net.cpp:165] label_data_1_split does not need backward computation.
I0313 19:39:02.070647 14018 net.cpp:165] data does not need backward computation.
I0313 19:39:02.070659 14018 net.cpp:201] This network produces output accuracy
I0313 19:39:02.070667 14018 net.cpp:201] This network produces output loss
I0313 19:39:02.070691 14018 net.cpp:446] Collecting Learning Rate and Weight Decay.
I0313 19:39:02.070709 14018 net.cpp:213] Network initialization done.
I0313 19:39:02.070724 14018 net.cpp:214] Memory required for data: 343014208
I0313 19:39:02.070839 14018 solver.cpp:42] Solver scaffolding done.
I0313 19:39:02.070879 14018 caffe.cpp:115] Finetuning from /root/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
E0313 19:39:02.690484 14018 upgrade_proto.cpp:609] Attempting to upgrade input file specified using deprecated transformation parameters: /root/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
I0313 19:39:02.759994 14018 upgrade_proto.cpp:612] Successfully upgraded file specified using deprecated data transformation parameters.
E0313 19:39:02.760025 14018 upgrade_proto.cpp:614] Note that future Caffe releases will only support transform_param messages for transformation fields.
E0313 19:39:02.760046 14018 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: /root/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
I0313 19:39:03.062064 14018 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0313 19:39:03.166975 14018 solver.cpp:222] Solving alexnet11
I0313 19:39:03.167040 14018 solver.cpp:223] Learning Rate Policy: step
I0313 19:39:03.167057 14018 solver.cpp:266] Iteration 0, Testing net (#0)
I0313 19:39:13.763814 14018 solver.cpp:315]     Test net output #0: accuracy = 0.034
I0313 19:39:13.763926 14018 solver.cpp:315]     Test net output #1: loss = 3.02831 (* 1 = 3.02831 loss)
I0313 19:39:14.467631 14018 solver.cpp:189] Iteration 0, loss = 3.3774
I0313 19:39:14.467711 14018 solver.cpp:204]     Train net output #0: loss = 3.3774 (* 1 = 3.3774 loss)
I0313 19:39:14.467743 14018 solver.cpp:470] Iteration 0, lr = 0.0001
I0313 19:40:39.804538 14018 solver.cpp:189] Iteration 100, loss = 0.504999
I0313 19:40:39.804703 14018 solver.cpp:204]     Train net output #0: loss = 0.504999 (* 1 = 0.504999 loss)
I0313 19:40:39.804733 14018 solver.cpp:470] Iteration 100, lr = 0.0001
I0313 19:42:04.927386 14018 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_200.caffemodel
I0313 19:42:05.872668 14018 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_200.solverstate
I0313 19:42:07.979571 14018 solver.cpp:266] Iteration 200, Testing net (#0)
I0313 19:42:18.425252 14018 solver.cpp:315]     Test net output #0: accuracy = 0.746667
I0313 19:42:18.425361 14018 solver.cpp:315]     Test net output #1: loss = 0.85692 (* 1 = 0.85692 loss)
I0313 19:42:19.104444 14018 solver.cpp:189] Iteration 200, loss = 0.539999
I0313 19:42:19.104529 14018 solver.cpp:204]     Train net output #0: loss = 0.539999 (* 1 = 0.539999 loss)
I0313 19:42:19.104559 14018 solver.cpp:470] Iteration 200, lr = 1e-05
I0313 19:43:44.426079 14018 solver.cpp:189] Iteration 300, loss = 0.693212
I0313 19:43:44.426307 14018 solver.cpp:204]     Train net output #0: loss = 0.693212 (* 1 = 0.693212 loss)
I0313 19:43:44.426337 14018 solver.cpp:470] Iteration 300, lr = 1e-05
I0313 19:45:09.519834 14018 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_400.caffemodel
I0313 19:45:11.394512 14018 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_400.solverstate
I0313 19:45:15.358098 14018 solver.cpp:266] Iteration 400, Testing net (#0)
I0313 19:45:25.826869 14018 solver.cpp:315]     Test net output #0: accuracy = 0.744667
I0313 19:45:25.826973 14018 solver.cpp:315]     Test net output #1: loss = 0.871993 (* 1 = 0.871993 loss)
I0313 19:45:26.506096 14018 solver.cpp:189] Iteration 400, loss = 0.530467
I0313 19:45:26.506184 14018 solver.cpp:204]     Train net output #0: loss = 0.530467 (* 1 = 0.530467 loss)
I0313 19:45:26.506202 14018 solver.cpp:470] Iteration 400, lr = 1e-06
I0313 19:46:51.822787 14018 solver.cpp:189] Iteration 500, loss = 0.362713
I0313 19:46:51.823030 14018 solver.cpp:204]     Train net output #0: loss = 0.362713 (* 1 = 0.362713 loss)
I0313 19:46:51.823050 14018 solver.cpp:470] Iteration 500, lr = 1e-06
I0313 19:48:16.895090 14018 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_600.caffemodel
I0313 19:48:17.707926 14018 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_600.solverstate
I0313 19:48:19.863309 14018 solver.cpp:266] Iteration 600, Testing net (#0)
I0313 19:48:30.307885 14018 solver.cpp:315]     Test net output #0: accuracy = 0.746333
I0313 19:48:30.307973 14018 solver.cpp:315]     Test net output #1: loss = 0.868095 (* 1 = 0.868095 loss)
I0313 19:48:30.986826 14018 solver.cpp:189] Iteration 600, loss = 0.246036
I0313 19:48:30.986928 14018 solver.cpp:204]     Train net output #0: loss = 0.246036 (* 1 = 0.246036 loss)
I0313 19:48:30.986963 14018 solver.cpp:470] Iteration 600, lr = 1e-07
I0313 19:49:56.298277 14018 solver.cpp:189] Iteration 700, loss = 0.511292
I0313 19:49:56.298530 14018 solver.cpp:204]     Train net output #0: loss = 0.511292 (* 1 = 0.511292 loss)
I0313 19:49:56.298552 14018 solver.cpp:470] Iteration 700, lr = 1e-07
I0313 19:51:21.366559 14018 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_800.caffemodel
I0313 19:51:23.173193 14018 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_800.solverstate
I0313 19:51:27.150950 14018 solver.cpp:266] Iteration 800, Testing net (#0)
I0313 19:51:37.591290 14018 solver.cpp:315]     Test net output #0: accuracy = 0.744333
I0313 19:51:37.591398 14018 solver.cpp:315]     Test net output #1: loss = 0.880087 (* 1 = 0.880087 loss)
I0313 19:51:38.270416 14018 solver.cpp:189] Iteration 800, loss = 0.269978
I0313 19:51:38.270494 14018 solver.cpp:204]     Train net output #0: loss = 0.269978 (* 1 = 0.269978 loss)
I0313 19:51:38.270514 14018 solver.cpp:470] Iteration 800, lr = 1e-08
I0313 19:53:03.575872 14018 solver.cpp:189] Iteration 900, loss = 0.305388
I0313 19:53:03.576112 14018 solver.cpp:204]     Train net output #0: loss = 0.305388 (* 1 = 0.305388 loss)
I0313 19:53:03.576133 14018 solver.cpp:470] Iteration 900, lr = 1e-08
I0313 19:54:28.649737 14018 solver.cpp:334] Snapshotting to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_1000.caffemodel
I0313 19:54:29.677276 14018 solver.cpp:342] Snapshotting solver state to /root/cs231n-project/cnns/alexnet-11/train_all_layers/results_exp1/snapshots/final/alexnet11_train_all_layers_iter_1000.solverstate
I0313 19:54:32.471539 14018 solver.cpp:266] Iteration 1000, Testing net (#0)
I0313 19:54:42.919759 14018 solver.cpp:315]     Test net output #0: accuracy = 0.747333
I0313 19:54:42.919858 14018 solver.cpp:315]     Test net output #1: loss = 0.861911 (* 1 = 0.861911 loss)
I0313 19:54:43.599361 14018 solver.cpp:189] Iteration 1000, loss = 0.381796
I0313 19:54:43.599454 14018 solver.cpp:204]     Train net output #0: loss = 0.381796 (* 1 = 0.381796 loss)
I0313 19:54:43.599475 14018 solver.cpp:470] Iteration 1000, lr = 1e-09
